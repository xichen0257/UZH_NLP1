{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "qcs8VCHENIdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bznwqi9-mdrZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_labels = pd.read_csv(\"mapping.txt\", sep=\"\\t\", names=['label', 'emotion'])"
      ],
      "metadata": {
        "id": "IpFOBrCemypI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set\n",
        "train_text = pd.read_csv(\"train_text.txt\", sep=\"\\t\", names=['text'])\n",
        "train_label = pd.read_csv(\"train_labels.txt\", sep=\"\\t\", names=['label'])\n",
        "train_df = pd.concat([train_text, train_label], axis=1)"
      ],
      "metadata": {
        "id": "j_BWtXf-n2hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation set\n",
        "val_text = pd.read_csv(\"val_text.txt\", sep=\"\\t\", names=['text'])\n",
        "val_label = pd.read_csv(\"val_labels.txt\", sep=\"\\t\", names=['label'])\n",
        "valid_df = pd.concat([val_text, val_label], axis=1)"
      ],
      "metadata": {
        "id": "eTdfkxVLpsl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set\n",
        "test_text = pd.read_csv(\"test_text.txt\", sep=\"\\t\", names=['text'])\n",
        "test_label = pd.read_csv(\"test_labels.txt\", sep=\"\\t\", names=['label'])\n",
        "test_df = pd.concat([test_text, test_label], axis=1)"
      ],
      "metadata": {
        "id": "FW0sZq_-pq0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pick 2 emotion classes**"
      ],
      "metadata": {
        "id": "v9BhIcxPqj87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly pick 3 emotions: emotion[0], emotion[1], emotion[2]\n",
        "# 1st dataframe contains emotion[0], emotion[1]\n",
        "# 2nd dataframe contains emotion[1], emotion[2]\n",
        "selected_emotions = emotion_labels.sample(n = 3)[\"label\"].tolist()"
      ],
      "metadata": {
        "id": "5wCnXRNbpqyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_train_df1 = train_df[train_df['label'].isin(selected_emotions[:2])]\n",
        "selected_valid_df1 = valid_df[valid_df['label'].isin(selected_emotions[:2])]\n",
        "selected_test_df1 = test_df[test_df['label'].isin(selected_emotions[:2])]\n",
        "\n",
        "selected_train_df2 = train_df[train_df['label'].isin(selected_emotions[1:])]\n",
        "selected_valid_df2 = valid_df[valid_df['label'].isin(selected_emotions[1:])]\n",
        "selected_test_df2 = test_df[test_df['label'].isin(selected_emotions[1:])]"
      ],
      "metadata": {
        "id": "xM1ZSE47pqt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess(texts):\n",
        "  # Lowercase\n",
        "  texts = texts.lower()\n",
        "  # Split tokens on white space\n",
        "  tokens = texts.split()\n",
        "  # Remove all punctuation from words\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "XkZbO2DI42Xg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c37d73-dbbe-45a0-9715-6fbebcfa3d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_train_df1[\"preprocess_text\"] = selected_train_df1[\"text\"].apply(preprocess)\n",
        "selected_valid_df1[\"preprocess_text\"] = selected_valid_df1[\"text\"].apply(preprocess)\n",
        "selected_test_df1[\"preprocess_text\"] = selected_test_df1[\"text\"].apply(preprocess)\n",
        "\n",
        "selected_train_df2[\"preprocess_text\"] = selected_train_df2[\"text\"].apply(preprocess)\n",
        "selected_valid_df2[\"preprocess_text\"] = selected_valid_df2[\"text\"].apply(preprocess)\n",
        "selected_test_df2[\"preprocess_text\"] = selected_test_df2[\"text\"].apply(preprocess)"
      ],
      "metadata": {
        "id": "ftIydqn95ni-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Works on 1st dataset**"
      ],
      "metadata": {
        "id": "kgfQJd6z8I-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "Kc3_O-ywJpzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Vocabulary\n",
        "vocab = Counter()\n",
        "\n",
        "all_train_tokens = []\n",
        "for row in selected_train_df1[\"preprocess_text\"].tolist():\n",
        "  for token in row:\n",
        "    all_train_tokens.append(token)\n",
        "\n",
        "vocab.update(all_train_tokens)"
      ],
      "metadata": {
        "id": "qGWUMLX0riUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text_to_sequence(tokens, max_length):\n",
        "  encoded = tokenizer.texts_to_sequences(tokens)\n",
        "  encoded = np.array(encoded).T.reshape(-1)\n",
        "  # Padding the encoded sentence to max_length, ensure all embeddings has the same length\n",
        "  pad_zero = np.zeros(max_length - encoded.shape[0])\n",
        "  output = np.concatenate((encoded, pad_zero))\n",
        "  return output\n",
        "\n",
        "# Create the tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(all_train_tokens)\n",
        "# define max_length for padding purpose\n",
        "max_length = 500\n",
        "# sequence encode\n",
        "selected_train_df1[\"encode_text\"] = selected_train_df1[\"preprocess_text\"].apply(lambda x: convert_text_to_sequence(x, max_length))\n",
        "selected_valid_df1[\"encode_text\"] = selected_valid_df1[\"preprocess_text\"].apply(lambda x: convert_text_to_sequence(x, max_length))\n",
        "selected_test_df1[\"encode_text\"] = selected_test_df1[\"preprocess_text\"].apply(lambda x: convert_text_to_sequence(x, max_length))\n",
        "\n",
        "# Padding = 0, <UNK> = 1"
      ],
      "metadata": {
        "id": "CNwuur6sriOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, max_length, filters, kernel_size, dropout_prob, hidden_dim):\n",
        "    super(CNN,self).__init__()\n",
        "    # Embedding layer\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    # Dropout\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    # Convolution layer\n",
        "    self.conv1d = nn.Conv1d(in_channels=embedding_dim, out_channels=filters, kernel_size=kernel_size)\n",
        "    # ReLu\n",
        "    self.relu = nn.ReLU()\n",
        "    # Global Max-Pooling\n",
        "    self.pool = nn.AdaptiveMaxPool1d(1)\n",
        "    # Fully connected layers\n",
        "    self.fc1 = nn.Linear(filters, hidden_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embeddings(x)\n",
        "    dropout = self.dropout(embedded)\n",
        "    dropout = dropout.permute(0, 2, 1)  # Conv1d expects (batch, channels, seq_len)\n",
        "    conv1d = self.conv1d(dropout)\n",
        "    relu = self.relu(conv1d)\n",
        "    pool = self.pool(relu).squeeze(-1)\n",
        "    fc1 = self.relu(self.dropout(self.fc1(pool)))\n",
        "    fc2 = self.fc2(fc1)\n",
        "    output = self.sigmoid(fc2)\n",
        "    return output"
      ],
      "metadata": {
        "id": "YlcFFVU7yIvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "OLD1z6HzQ8mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data\n",
        "train_text_data = list(selected_train_df1[\"encode_text\"])\n",
        "\n",
        "train_label_data = list(selected_train_df1[\"label\"])\n",
        "selected_label = set(selected_train_df1[\"label\"])\n",
        "# Map the label to {0,1}, because we will do Binary-cross-entropy later, only accept label=[0,1]\n",
        "# dict = {Label1 : 0, Label2: 1}\n",
        "mapping_dict = {value: index for index, value in enumerate(selected_label)}\n",
        "train_label_data = list(mapping_dict[value] for value in train_label_data)\n",
        "\n",
        "Xtrain = torch.LongTensor(train_text_data)\n",
        "Ytrain = torch.Tensor(train_label_data)"
      ],
      "metadata": {
        "id": "fFiTa8mYbw_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Valid Data\n",
        "valid_text_data = list(selected_valid_df1[\"encode_text\"])\n",
        "\n",
        "valid_label_data = list(selected_valid_df1[\"label\"])\n",
        "valid_label_data = list(mapping_dict[value] for value in valid_label_data)\n",
        "\n",
        "# transform to torch.LongTensor(integer)\n",
        "Xvalid = torch.LongTensor(valid_text_data)\n",
        "Yvalid = torch.Tensor(valid_label_data)"
      ],
      "metadata": {
        "id": "DTYACB8EBb3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Data\n",
        "test_text_data = list(selected_test_df1[\"encode_text\"])\n",
        "\n",
        "test_label_data = list(selected_test_df1[\"label\"])\n",
        "test_label_data = list(mapping_dict[value] for value in test_label_data)\n",
        "\n",
        "# transform to torch.LongTensor(integer)\n",
        "Xtest = torch.LongTensor(test_text_data)\n",
        "Ytest = torch.Tensor(test_label_data)"
      ],
      "metadata": {
        "id": "oV8no_xvDiY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate_model(Xtrain, Ytrain, Xvalid, Yvalid, Xtest, Ytest, vocab_size, batch_sizes, epochs, learning_rate):\n",
        "  print(f'Train & Evaluate model with batch_sizes={batch_sizes}; epochs={epochs}; learning_rate={learning_rate}')\n",
        "  # Hyperparameter that we fixed:\n",
        "  embedding_dim = 50\n",
        "  filters = 250\n",
        "  kernel_size = 3\n",
        "  hidden_dim = 250\n",
        "  dropout_prob = 0.2\n",
        "\n",
        "  train_dataset = TensorDataset(Xtrain, Ytrain)\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_sizes, shuffle=True)\n",
        "\n",
        "  # Initialize model and optimizer\n",
        "  model = CNN(vocab_size, embedding_dim, max_length, filters, kernel_size, dropout_prob, hidden_dim)\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for X, Y in train_dataloader:\n",
        "      optimizer.zero_grad()\n",
        "      output = model(X).squeeze(-1)\n",
        "      loss = criterion(output, Y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "      validation_loss = 0\n",
        "      v_output = model(Xvalid).squeeze(-1)\n",
        "      v_loss = criterion(v_output, Yvalid)\n",
        "      validation_loss = v_loss.item()\n",
        "\n",
        "    print(f'[Train] Epoch [{epoch + 1}/{epochs}], Training Loss: {total_loss:.4f}, Validation Loss: {validation_loss:.4f}')\n",
        "\n",
        "  # Evaluate with Testing set\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_outputs = model(Xtest).squeeze(-1)\n",
        "    test_outputs = (test_outputs > 0.5).float()\n",
        "    accuracy = accuracy_score(Ytest, test_outputs)\n",
        "    f1score = f1_score(Ytest, test_outputs, average='macro')\n",
        "    print(f'[Test] Accuracy: {accuracy * 100:.4f}%, F1-score: {f1score * 100:.4f}%\\n')\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "et5PkPlIQf_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [30, 50, 70]\n",
        "epochs = [3, 4, 5]\n",
        "learning_rate = [0.001, 0.002]\n",
        "best_batch = batch_sizes[0]\n",
        "best_epochs = epochs[0]\n",
        "best_lr = learning_rate[0]\n",
        "best_acc = 0\n",
        "\n",
        "for batch in batch_sizes:\n",
        "  for epoch in epochs:\n",
        "    for lr in learning_rate:\n",
        "      acc = train_evaluate_model(Xtrain, Ytrain, Xvalid, Yvalid, Xtest, Ytest, vocab_size, batch, epoch, lr)\n",
        "      if acc >= best_acc:\n",
        "        best_acc = acc\n",
        "        best_batch = batch\n",
        "        best_epochs = epoch\n",
        "        best_lr = lr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66_jj_Du997Y",
        "outputId": "a3e86156-0379-4c41-fef2-2ea1ce2a0024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train & Evaluate model with batch_sizes=30; epochs=3; learning_rate=0.001\n",
            "[Train] Epoch [1/3], Training Loss: 22.1502, Validation Loss: 0.5526\n",
            "[Train] Epoch [2/3], Training Loss: 21.9226, Validation Loss: 0.5458\n",
            "[Train] Epoch [3/3], Training Loss: 19.7119, Validation Loss: 0.5562\n",
            "[Test] Accuracy: 77.4257%, F1-score: 62.1210%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=30; epochs=3; learning_rate=0.002\n",
            "[Train] Epoch [1/3], Training Loss: 23.3457, Validation Loss: 0.5654\n",
            "[Train] Epoch [2/3], Training Loss: 20.8139, Validation Loss: 0.5384\n",
            "[Train] Epoch [3/3], Training Loss: 16.7957, Validation Loss: 0.6700\n",
            "[Test] Accuracy: 76.0396%, F1-score: 46.2336%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=30; epochs=4; learning_rate=0.001\n",
            "[Train] Epoch [1/4], Training Loss: 23.1334, Validation Loss: 0.5739\n",
            "[Train] Epoch [2/4], Training Loss: 21.7531, Validation Loss: 0.5557\n",
            "[Train] Epoch [3/4], Training Loss: 20.3962, Validation Loss: 0.5339\n",
            "[Train] Epoch [4/4], Training Loss: 17.6042, Validation Loss: 0.5489\n",
            "[Test] Accuracy: 73.2673%, F1-score: 62.3428%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=30; epochs=4; learning_rate=0.002\n",
            "[Train] Epoch [1/4], Training Loss: 23.6250, Validation Loss: 0.5749\n",
            "[Train] Epoch [2/4], Training Loss: 20.8979, Validation Loss: 0.5531\n",
            "[Train] Epoch [3/4], Training Loss: 18.5153, Validation Loss: 0.5159\n",
            "[Train] Epoch [4/4], Training Loss: 12.2329, Validation Loss: 0.4990\n",
            "[Test] Accuracy: 77.0297%, F1-score: 62.9952%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=30; epochs=5; learning_rate=0.001\n",
            "[Train] Epoch [1/5], Training Loss: 22.3053, Validation Loss: 0.5842\n",
            "[Train] Epoch [2/5], Training Loss: 22.1463, Validation Loss: 0.5809\n",
            "[Train] Epoch [3/5], Training Loss: 20.2570, Validation Loss: 0.6719\n",
            "[Train] Epoch [4/5], Training Loss: 17.6473, Validation Loss: 0.5100\n",
            "[Train] Epoch [5/5], Training Loss: 14.2368, Validation Loss: 0.4905\n",
            "[Test] Accuracy: 74.6535%, F1-score: 63.7457%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=30; epochs=5; learning_rate=0.002\n",
            "[Train] Epoch [1/5], Training Loss: 23.2963, Validation Loss: 0.5606\n",
            "[Train] Epoch [2/5], Training Loss: 21.2701, Validation Loss: 0.5810\n",
            "[Train] Epoch [3/5], Training Loss: 17.5343, Validation Loss: 0.5314\n",
            "[Train] Epoch [4/5], Training Loss: 11.6645, Validation Loss: 0.5102\n",
            "[Train] Epoch [5/5], Training Loss: 6.0070, Validation Loss: 0.5226\n",
            "[Test] Accuracy: 76.4356%, F1-score: 61.8882%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=50; epochs=3; learning_rate=0.001\n",
            "[Train] Epoch [1/3], Training Loss: 13.6340, Validation Loss: 0.5738\n",
            "[Train] Epoch [2/3], Training Loss: 12.9178, Validation Loss: 0.5711\n",
            "[Train] Epoch [3/3], Training Loss: 12.5557, Validation Loss: 0.5734\n",
            "[Test] Accuracy: 75.8416%, F1-score: 43.9216%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=50; epochs=3; learning_rate=0.002\n",
            "[Train] Epoch [1/3], Training Loss: 14.0786, Validation Loss: 0.5590\n",
            "[Train] Epoch [2/3], Training Loss: 12.7408, Validation Loss: 0.5941\n",
            "[Train] Epoch [3/3], Training Loss: 11.6918, Validation Loss: 0.5184\n",
            "[Test] Accuracy: 76.8317%, F1-score: 59.9263%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=50; epochs=4; learning_rate=0.001\n",
            "[Train] Epoch [1/4], Training Loss: 13.3537, Validation Loss: 0.5618\n",
            "[Train] Epoch [2/4], Training Loss: 13.0253, Validation Loss: 0.5714\n",
            "[Train] Epoch [3/4], Training Loss: 12.2566, Validation Loss: 0.5648\n",
            "[Train] Epoch [4/4], Training Loss: 11.2823, Validation Loss: 0.5505\n",
            "[Test] Accuracy: 75.4455%, F1-score: 53.4825%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=50; epochs=4; learning_rate=0.002\n",
            "[Train] Epoch [1/4], Training Loss: 14.1934, Validation Loss: 0.5792\n",
            "[Train] Epoch [2/4], Training Loss: 12.8610, Validation Loss: 0.5524\n",
            "[Train] Epoch [3/4], Training Loss: 11.9028, Validation Loss: 0.5298\n",
            "[Train] Epoch [4/4], Training Loss: 9.5641, Validation Loss: 0.4950\n",
            "[Test] Accuracy: 76.6337%, F1-score: 54.3078%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=50; epochs=5; learning_rate=0.001\n",
            "[Train] Epoch [1/5], Training Loss: 14.1694, Validation Loss: 0.6067\n",
            "[Train] Epoch [2/5], Training Loss: 13.2184, Validation Loss: 0.5885\n",
            "[Train] Epoch [3/5], Training Loss: 12.6325, Validation Loss: 0.5737\n",
            "[Train] Epoch [4/5], Training Loss: 11.9754, Validation Loss: 0.5485\n",
            "[Train] Epoch [5/5], Training Loss: 10.7839, Validation Loss: 0.5328\n",
            "[Test] Accuracy: 76.2376%, F1-score: 54.5127%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=50; epochs=5; learning_rate=0.002\n",
            "[Train] Epoch [1/5], Training Loss: 13.8315, Validation Loss: 0.5597\n",
            "[Train] Epoch [2/5], Training Loss: 12.8051, Validation Loss: 0.5551\n",
            "[Train] Epoch [3/5], Training Loss: 11.8053, Validation Loss: 0.5207\n",
            "[Train] Epoch [4/5], Training Loss: 9.2932, Validation Loss: 0.4658\n",
            "[Train] Epoch [5/5], Training Loss: 6.2924, Validation Loss: 0.4714\n",
            "[Test] Accuracy: 72.2772%, F1-score: 62.9874%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=70; epochs=3; learning_rate=0.001\n",
            "[Train] Epoch [1/3], Training Loss: 10.1162, Validation Loss: 0.5604\n",
            "[Train] Epoch [2/3], Training Loss: 9.5816, Validation Loss: 0.5677\n",
            "[Train] Epoch [3/3], Training Loss: 9.5069, Validation Loss: 0.5461\n",
            "[Test] Accuracy: 75.6436%, F1-score: 43.0665%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=70; epochs=3; learning_rate=0.002\n",
            "[Train] Epoch [1/3], Training Loss: 10.7012, Validation Loss: 0.6003\n",
            "[Train] Epoch [2/3], Training Loss: 9.7680, Validation Loss: 0.5933\n",
            "[Train] Epoch [3/3], Training Loss: 9.1711, Validation Loss: 0.5754\n",
            "[Test] Accuracy: 75.4455%, F1-score: 43.7801%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=70; epochs=4; learning_rate=0.001\n",
            "[Train] Epoch [1/4], Training Loss: 9.7861, Validation Loss: 0.5896\n",
            "[Train] Epoch [2/4], Training Loss: 9.5161, Validation Loss: 0.5867\n",
            "[Train] Epoch [3/4], Training Loss: 9.2573, Validation Loss: 0.5822\n",
            "[Train] Epoch [4/4], Training Loss: 8.9375, Validation Loss: 0.5405\n",
            "[Test] Accuracy: 75.8416%, F1-score: 44.6868%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=70; epochs=4; learning_rate=0.002\n",
            "[Train] Epoch [1/4], Training Loss: 10.5574, Validation Loss: 0.6066\n",
            "[Train] Epoch [2/4], Training Loss: 9.9663, Validation Loss: 0.5674\n",
            "[Train] Epoch [3/4], Training Loss: 9.4774, Validation Loss: 0.5612\n",
            "[Train] Epoch [4/4], Training Loss: 8.8277, Validation Loss: 0.5750\n",
            "[Test] Accuracy: 76.0396%, F1-score: 53.3980%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=70; epochs=5; learning_rate=0.001\n",
            "[Train] Epoch [1/5], Training Loss: 9.7795, Validation Loss: 0.5505\n",
            "[Train] Epoch [2/5], Training Loss: 9.6348, Validation Loss: 0.5565\n",
            "[Train] Epoch [3/5], Training Loss: 9.3542, Validation Loss: 0.5630\n",
            "[Train] Epoch [4/5], Training Loss: 8.8798, Validation Loss: 0.5497\n",
            "[Train] Epoch [5/5], Training Loss: 8.1932, Validation Loss: 0.5274\n",
            "[Test] Accuracy: 78.0198%, F1-score: 57.2494%\n",
            "\n",
            "Train & Evaluate model with batch_sizes=70; epochs=5; learning_rate=0.002\n",
            "[Train] Epoch [1/5], Training Loss: 10.9134, Validation Loss: 0.5550\n",
            "[Train] Epoch [2/5], Training Loss: 9.6409, Validation Loss: 0.5577\n",
            "[Train] Epoch [3/5], Training Loss: 9.0466, Validation Loss: 0.5423\n",
            "[Train] Epoch [4/5], Training Loss: 7.7449, Validation Loss: 0.6765\n",
            "[Train] Epoch [5/5], Training Loss: 6.3393, Validation Loss: 0.6980\n",
            "[Test] Accuracy: 56.2376%, F1-score: 53.9737%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Best Hyperparams Combination:')\n",
        "print(f'batch_sizes = {best_batch}')\n",
        "print(f'epochs = {best_epochs}')\n",
        "print(f'learning_rate = {best_lr}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH54otXzafYM",
        "outputId": "9c259e47-2317-483a-b5c7-535eb920b5df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparams Combination:\n",
            "batch_sizes = 70\n",
            "epochs = 5\n",
            "learning_rate = 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Work with 2nd Dataset**"
      ],
      "metadata": {
        "id": "ppsVkCXJIwvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Vocabulary\n",
        "vocab = Counter()\n",
        "\n",
        "all_train_tokens = []\n",
        "for row in selected_train_df2[\"preprocess_text\"].tolist():\n",
        "  for token in row:\n",
        "    all_train_tokens.append(token)\n",
        "\n",
        "vocab.update(all_train_tokens)"
      ],
      "metadata": {
        "id": "7v1j64wgIze9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(all_train_tokens)\n",
        "# define max_length for padding purpose\n",
        "max_length = 500\n",
        "# sequence encode\n",
        "selected_train_df2[\"encode_text\"] = selected_train_df2[\"preprocess_text\"].apply(lambda x: convert_text_to_sequence(x, max_length))\n",
        "selected_valid_df2[\"encode_text\"] = selected_valid_df2[\"preprocess_text\"].apply(lambda x: convert_text_to_sequence(x, max_length))\n",
        "selected_test_df2[\"encode_text\"] = selected_test_df2[\"preprocess_text\"].apply(lambda x: convert_text_to_sequence(x, max_length))\n",
        "\n",
        "# Padding = 0, <UNK> = 1"
      ],
      "metadata": {
        "id": "l8wq9HmeIzcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "ekpbh9IbaRNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data\n",
        "train_text_data = list(selected_train_df2[\"encode_text\"])\n",
        "\n",
        "train_label_data = list(selected_train_df2[\"label\"])\n",
        "selected_label = set(selected_train_df2[\"label\"])\n",
        "# Map the label to {0,1}, because we will do Binary-cross-entropy later, only accept label=[0,1]\n",
        "# dict = {Label1 : 0, Label2: 1}\n",
        "mapping_dict = {value: index for index, value in enumerate(selected_label)}\n",
        "train_label_data = list(mapping_dict[value] for value in train_label_data)\n",
        "\n",
        "Xtrain = torch.LongTensor(train_text_data)\n",
        "Ytrain = torch.Tensor(train_label_data)"
      ],
      "metadata": {
        "id": "4FtEmqxxIzW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Valid Data\n",
        "valid_text_data = list(selected_valid_df2[\"encode_text\"])\n",
        "\n",
        "valid_label_data = list(selected_valid_df2[\"label\"])\n",
        "valid_label_data = list(mapping_dict[value] for value in valid_label_data)\n",
        "\n",
        "# transform to torch.LongTensor(integer)\n",
        "Xvalid = torch.LongTensor(valid_text_data)\n",
        "Yvalid = torch.Tensor(valid_label_data)"
      ],
      "metadata": {
        "id": "J5kyN0RYIzPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Data\n",
        "test_text_data = list(selected_test_df2[\"encode_text\"])\n",
        "\n",
        "test_label_data = list(selected_test_df2[\"label\"])\n",
        "test_label_data = list(mapping_dict[value] for value in test_label_data)\n",
        "\n",
        "# transform to torch.LongTensor(integer)\n",
        "Xtest = torch.LongTensor(test_text_data)\n",
        "Ytest = torch.Tensor(test_label_data)"
      ],
      "metadata": {
        "id": "qUi2kmRAKOfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = train_evaluate_model(Xtrain, Ytrain, Xvalid, Yvalid, Xtest, Ytest, vocab_size, best_batch, best_epochs, best_lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlj-_ePCKWDq",
        "outputId": "7fb7f456-d4f9-42ea-897a-6f3ccb4dc9c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train & Evaluate model with batch_sizes=70; epochs=5; learning_rate=0.001\n",
            "[Train] Epoch [1/5], Training Loss: 9.2783, Validation Loss: 0.6084\n",
            "[Train] Epoch [2/5], Training Loss: 9.0108, Validation Loss: 0.6164\n",
            "[Train] Epoch [3/5], Training Loss: 8.7736, Validation Loss: 0.5697\n",
            "[Train] Epoch [4/5], Training Loss: 8.7118, Validation Loss: 0.5651\n",
            "[Train] Epoch [5/5], Training Loss: 7.9673, Validation Loss: 0.5799\n",
            "[Test] Accuracy: 69.8545%, F1-score: 60.0812%\n",
            "\n"
          ]
        }
      ]
    }
  ]
}